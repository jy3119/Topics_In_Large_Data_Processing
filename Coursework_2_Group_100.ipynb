{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2\n",
    "\n",
    "Problems are set by Dr. Wei Dai and Miss Farwa Abbas, 2022\n",
    "\n",
    "Instructions:\n",
    "\n",
    "While submitting your coursework please ensure that you rename the file as `Coursework_x_Group_x.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, FFTW, Wavelets, Images, LinearAlgebra, Random, StatsBase\n",
    "\n",
    "# Make sure that you complete the involvement table. \n",
    "# The first row is for CID number. \n",
    "# Other rows are for the involvement for each \"big\" problem (8 big problems in coursework 1). \n",
    "# \"1\" for getting involved in this part. \n",
    "# \"0\" for not involved\n",
    "Contributions = DataFrame( A=[01700345,1,1,1,1,1,1,1,1,1,1], B = [01518690,1,1,1,1,1,1,1,1,1,1], C = [02243990, 1,1,1,1,1,1,1,1,1,1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (6%)\n",
    "\n",
    "1. Let $A,B$ be two convex sets. Define $C:= A+B = \\left\\{ \\boldsymbol{a} + \\boldsymbol{b} |~ \\boldsymbol{a} \\in A,~ \\boldsymbol{b} \\in B \\right\\}$. Show that $C$ is convex.\n",
    "2. Let $A_k$, $k=1,2,\\cdots,K$, be convex sets. Show that $A := \\bigcap_{k=1}^K A_k$ is convex. \n",
    "3. Show that a set is convex if and only if its intersection with any line is convex. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. $\\forall a_{1},a_{2}\\in A,\\  \\forall \\lambda \\in \\left[ 0,1\\right]  ,\\  \\lambda a_{1}+(1-\\lambda )a_{2}\\in A$....(1)\n",
    "\n",
    "   $\\forall b_{1},b_{2}\\in B,\\  \\forall \\lambda \\in \\left[ 0,1\\right]  ,\\  \\lambda b_{1}+(1-\\lambda )b_{2}\\in B$....(2)\n",
    "\n",
    "   (1) + (2) $\\lambda (a_{1}+b_{1})+(1-\\lambda )(a_{2}+b_{2})$\n",
    "   \n",
    "   Let $\\ a_{1}+b_{1} = c_{1}$   $\\ a_{2}+b_{2} = c_{2}$\n",
    "\n",
    "   $\\lambda c_{1}+(1-\\lambda )c_{2}\\in C$\n",
    "\n",
    "2. $\\forall a,b \\in X\\bigcap Y$, all the points in $\\overline{ab}$ belong to X and Y. So all the points in  $\\overline{ab}$ belong to $X\\bigcap Y$. As a,b is sellected randomly in $X\\bigcap Y$, $X\\bigcap Y$ is convex. And so on, $A := \\bigcap_{k=1}^K A_k$ is convex. \n",
    "\n",
    "3. Sufficiency: As intersections of convex sets are convex, intersections of convex sets and lines are convex.  \n",
    "   Necessity: $\\forall$ x,y on the lines, $x\\bigcap y$ is convex. As lines lie randomly on a certain set, this set is convex.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (3%)\n",
    "\n",
    "This question is about the distance between two parallel hyperplanes $\\left\\{ \\boldsymbol{x} \\in \\mathbb{R}^n | \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} = b_1 \\right\\}$ and $\\left\\{ x \\in \\mathbb{R}^n | \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} = b_2 \\right\\}$. \n",
    "\n",
    "1.  Give a scientifically reasonable definition for the distance. \n",
    "2. Find the distance in a closed form. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. Find a point on two parallel hyperplanes respectively. The minimum distance between the two points is the distance between the two parallel hyperplanes. \n",
    "\n",
    "2.a is the normal vector of hyperplanes. $x_{1}$ is a point on the first hyperplane. The normal through $x_{1}$ is $x=x_{1}+ta$. $x_{2}$ is a point on the second hyperplane. In order to get $x_{2}$, we can have $a^{T}\\left( x_{1}+ta\\right)  =b_{2}$. So, $t=\\frac{b_{2}-a^{T}x_{1}}{a^{T}a} $, $x_{2}=x_{1}+\\frac{\\left( b_{2}-a^{T}x_{1}\\right)  a}{a^{T}a} =x_{1}+\\frac{\\left( b_{2}-b_{1}\\right)  a}{a^{T}a} $. The distance between the two parallel hyperplanes is the distance between $x_{1}$ and $x_{2}$. \n",
    "  $\\| x_{2}-x_{1}\\| =\\| \\frac{\\left( b_{2}-b_{1}\\right)  a}{a^{T}a} \\| =\\frac{|b_{2}-b_{1}|}{\\| a\\| } $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 (12%)\n",
    "\n",
    "For each of the following sets, indicate whether it is convex or not and prove your answer.\n",
    "\n",
    "1.  A slab, i.e., $\\{ \\boldsymbol{x} \\in \\mathbb{R}^n  | \\alpha \\le \\boldsymbol{a}^{\\mathsf{T}} \\boldsymbol{x} \\le \\beta \\}$. \n",
    "        \n",
    "2. A rectangle, i.e., $\\{ \\boldsymbol{x} \\in \\mathbb{R}^n  | \\alpha_i \\le x_i  \\le \\beta_i \\}$.\n",
    "        \n",
    "3. The set of points closer to a given point $x_0$ than to another given point $\\boldsymbol{y}$, i.e., \n",
    "        $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\lVert \\boldsymbol{x} - x_0 \\rVert _2 \\le \n",
    "                \\lVert \\boldsymbol{x} - \\boldsymbol{y} \\rVert _2\n",
    "            \\right\\}.\n",
    "        \\end{align*}$$\n",
    "        \n",
    "4. The set of points closer to a given point than to a given set, i.e., $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\lVert \\boldsymbol{x} - x_0 \\rVert _2 \\le \n",
    "                \\lVert \\boldsymbol{x} - \\boldsymbol{y} \\rVert _2,\n",
    "                ~ \\forall \\boldsymbol{y} \\in S\n",
    "            \\right\\}\n",
    "        \\end{align*}$$\n",
    "        where $S \\subseteq \\mathbb{R}^n$. \n",
    "        \n",
    "5. The set of points closer to one set than another, i.e., $$\\begin{align*}\n",
    "            & \\left\\{ \n",
    "                \\boldsymbol{x} |  \n",
    "                \\text{dist}(\\boldsymbol{x},S) \\le \\text{dist}(\\boldsymbol{x},T)\n",
    "            \\right\\}\n",
    "        \\end{align*}$$\n",
    "        where $$\\begin{align*}\n",
    "            \\text{dist}(\\boldsymbol{x},S) := \\text{inf}\\{ \\lVert x - \\boldsymbol{z} \\rVert_2 ~\\mid~ \n",
    "            \\boldsymbol{z} \\in S \\}. \n",
    "        \\end{align*}$$\n",
    "\n",
    "6. The set of points whose distance to $\\boldsymbol{a}$ does not exceed a fixed fraction $\\theta$ of the distance to $\\boldsymbol{b}$, i.e., the set $\\{ \\boldsymbol{x} \\mid ~ \\lVert \\boldsymbol{x} - \\boldsymbol{a} \\rVert_2 \\le \\theta \\lVert \\boldsymbol{x} - \\boldsymbol{b} \\rVert_2 \\}$, where $\\boldsymbol{a} \\ne \\boldsymbol{b}$ and $0 \\le \\theta \\le 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is an intersection of two halfspaces, so it is a convex.\n",
    "2. It is a finite interation of halfspaces, so it is a convex.\n",
    "3. We have $\\left| \\left| x-a\\right|  \\right|_{2}  \\leqslant \\left| \\left| x-b\\right|  \\right|_{2}  $, so we can get $\\left| \\left| x-a\\right|  \\right|^{2}_{2}  \\leqslant \\left| \\left| x-b\\right|  \\right|^{2}_{2}  $.\n",
    "\n",
    "$\\left( x-a\\right)^{T}  \\left( x-a\\right)  \\leqslant \\left( x-b\\right)^{T}  \\left( x-b\\right)  $\n",
    "\n",
    "$x^{T}x-2a^{T}x+a^{T}a\\leq x^{T}x-2b^{T}x+b^{T}b$\n",
    "\n",
    "$2\\left( b-a\\right)^{T}  x\\leqslant b^{T}b-a^{T}a$\n",
    "Let $c=2(b-a)$ and $d=b^{T}b-a^{T}a$\n",
    "\n",
    "Then we can get $cx\\leqslant d$\n",
    "\n",
    "So, we can find that it is a halfspace.\n",
    "\n",
    "4. It is convex because it is an intersection of halfspaces in 3.\n",
    "$\\bigcap_{y\\in S} \\{ x|\\left| \\left| x-x_{0}\\right|  \\right|_{2}  \\leqslant \\left| \\left| x-y\\right|  \\right|_{2}  \\} $\n",
    "\n",
    "5. It is not convex. Iet S={-1,1} and T={0}, we can get $\\left\\{ x|dist(x,S)\\leq dist(x,T)\\right\\}  =\\left\\{ x\\in R\\mid x\\leq -1\\  or\\  x\\geq 1\\right\\}  $ , so it is not convex.\n",
    "\n",
    "6. It is convex. $\\left\\{ x|\\left| \\left| x-a\\right|  \\right|_{2}  \\leq \\theta \\left| \\left| x-b\\right|  \\right|_{2}  \\right\\}  =\\left\\{ x|\\left| \\left| x-a\\right|  \\right|^{2}_{2}  \\leq \\theta \\left| \\left| x-b\\right|  \\right|^{2}_{2}  \\right\\}  =\\left\\{ x|\\left( 1-\\theta^{2} \\right)  x^{T}x-2(a-\\theta^{2} b)^{T}x+\\left( a^{T}a-\\theta^{2} b^{T}b\\right)  \\leq 0\\right\\}  $\n",
    "\n",
    "if $\\theta =1$ , it is a halfspase, so it is convex.\n",
    "\n",
    "if $\\theta \\leq 1$ , let $x_{0}=\\frac{a-\\theta^{2} b}{1-\\theta^{2} } $ and \n",
    "$R=\\sqrt{\\left( \\frac{\\theta^{2} \\left| \\left| b\\right|  \\right|^{2}_{2}  -\\left| \\left| a\\right|  \\right|^{2}_{2}  }{1-\\theta^{2} } \\right)  +\\left| \\left| x_{0}\\right|  \\right|^{2}_{2}  } $ , we can get \n",
    "$\\left\\{ x|\\  \\left( x-x_{0}\\right)^{T}  \\left( x-x_{0}\\right)  \\leq R^{2}\\right\\}  $ , so it is a ball, which is convex. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Functions\n",
    "\n",
    "### 4 (5%)\n",
    "\n",
    "Use the second-order condition of convexity to prove the following functions are convex \n",
    "\n",
    "1.  $f(x) = - \\log(x)$ where $x \\in \\mathbb{R}^+$.\n",
    "2. $f(x) = x \\log (x)$ where $x \\in \\mathbb{R}^+$.\n",
    "3. Affine functions $f(\\boldsymbol{x}) = \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b}$. \n",
    "4. Quadratic functions $f(\\boldsymbol{x}) = \\frac{1}{2} \\boldsymbol{x}^{\\mathsf{T}} \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b}^{\\mathsf{T}} \\boldsymbol{x} + c$ where $\\boldsymbol{A} \\succeq 0$. \n",
    "5. $f(\\boldsymbol{x}) = \\frac{1}{2} \\lVert \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b} \\rVert_2^2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "1.\n",
    "$f(x) = - \\log(x)$ where $x \\in \\mathbb{R}^+$.\n",
    "\n",
    "$\\nabla f(x) = -\\frac{1}{x} $\n",
    "\n",
    "$\\nabla^{2} f(x) = \\frac{1}{x^2}$\n",
    "\n",
    "For $x \\in \\mathbb{R}^+$, \n",
    "$\\frac{1}{x^2} \\geq 0$. Therefore, according to the second order condition of convexity, $f(x)$ is convex.\n",
    "\n",
    "\n",
    "2.\n",
    "$f(x) = x \\log (x)$ where $x \\in \\mathbb{R}^+$.\n",
    "\n",
    "$\\nabla f(x) = x(\\frac{1}{x}) + \\log(x) = 1 + \\log(x)$\n",
    "\n",
    "$\\nabla^{2} f(x) = \\frac{1}{x}$\n",
    "\n",
    "For $x \\in \\mathbb{R}^+$, \n",
    "$\\frac{1}{x} \\geq 0$. Therefore, according to the second order condition of convexity, $f(x)$ is convex.\n",
    "\n",
    "3.\n",
    "Affine functions $f(\\boldsymbol{x}) = \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b}$.\n",
    "\n",
    "Taking the first derivative,\n",
    "\n",
    "$\\nabla f(x) = \\boldsymbol{A}$\n",
    "\n",
    "$\\nabla^2 f(x) = 0$\n",
    "\n",
    "Since the second derivative = 0, it is both concave and convex.\n",
    "\n",
    "For $\\forall \\lambda \\in [0,1]$, \n",
    "\n",
    "We find \n",
    "\n",
    "$f(\\lambda x_1 + (1-\\lambda)x_2) = \\boldsymbol{A} (\\lambda x_1 + (1-\\lambda)x_2) + \\boldsymbol{b}$\n",
    "\n",
    "$= \\lambda \\boldsymbol{A}x_1 + (1-\\lambda)\\boldsymbol{A}x_2+ \\lambda b + (1-\\lambda) b $\n",
    "\n",
    "$= \\lambda f(x_1) + (1-\\lambda) f(x_2)$\n",
    "\n",
    "Since $f(\\lambda x_1 + (1-\\lambda)x_2) = \\boldsymbol{A} (\\lambda x_1 + (1-\\lambda)x_2) + \\boldsymbol{b}$ is both $\\geq$ or $\\leq$ with $\\lambda f(x_1) + (1-\\lambda) f(x_2)$, the affine function is both convex and concave.\n",
    "\n",
    "4.\n",
    "$f(\\boldsymbol{x}) = \\frac{1}{2} \\boldsymbol{x}^{\\mathsf{T}} \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b}^{\\mathsf{T}} \\boldsymbol{x} + c$ where $\\boldsymbol{A} \\succeq 0$. \n",
    "\n",
    "as a Taylor expansion,\n",
    "$f(x) = \\frac{1}{2} \\boldsymbol{x}^{\\mathsf{T}}f''(0)x + f'(0)x + f(0)$,\n",
    "\n",
    "giving $\\frac{1}{2} f''(0) = \\frac{1}{2} \\boldsymbol{A}$ ,\n",
    "thus $f''(0) = \\boldsymbol{A}$.\n",
    "\n",
    "Since it is given that $\\boldsymbol{A} \\succeq 0$, $f''(0) \\succeq 0$. \n",
    "\n",
    "Therefore,according to the second order condition of convexity, quadratic function $f(x)$ is convex given the condition $\\boldsymbol{A} \\succeq 0$.\n",
    "\n",
    "5.\n",
    "$f(\\boldsymbol{x}) = \\frac{1}{2} \\lVert \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b} \\rVert_2^2$ \n",
    "$= \\frac{1}{2} \\Sigma (a_{i}x_{i} + b_{i})^2$\n",
    "$= \\frac{1}{2} \\Sigma (a_{i}^2 x_{i}^2 + 2a_{i}b_{i} + b_{i}^2)$\n",
    "\n",
    "$\\nabla f(\\boldsymbol{x}) = \\Sigma a_{i}^2x_{i}$\n",
    "\n",
    "$\\nabla^2 f(\\boldsymbol{x}) = \\Sigma a_{i}^2$\n",
    "\n",
    "Since $a_{i}^2$ always $\\geq 0$, $\\Sigma a_{i}^2 \\geq 0$. Therefore, according to the second order condition of convexity, $f(\\boldsymbol{x})$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5 (8%)\n",
    "\n",
    "1.  Prove that if $f(\\boldsymbol{x})$ is convex, then $g(\\boldsymbol{x}) := f(\\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b})$ is convex. \n",
    "        \n",
    "2.  Prove that any norm $\\lVert \\cdot \\rVert$ on $\\mathbb{R}^m$ is convex. \n",
    "        \n",
    "3. Let $$\\begin{align*}\n",
    "            f(\\boldsymbol{x}) := \\max_{i=1,\\cdots,k}~ \n",
    "            \\lVert  \\boldsymbol{A}^{(i)} \\boldsymbol{x} + \\boldsymbol{b}^{(i)} \\rVert, \n",
    "        \\end{align*}$$\n",
    "    where $\\boldsymbol{A}^{(i)} \\in \\mathbb{R}^{m \\times n}$, $\\boldsymbol{b}^{(i)} \\in \\mathbb{R}^m$, and $\\lVert \\cdot \\rVert$ is a norm on $\\mathbb{R}^m$. Indicate whether $f(\\cdot)$ is convex or not. Prove your claim. \n",
    "        \n",
    "4. Let $$\\begin{align*}\n",
    "            f(\\boldsymbol{x}) = \\sum_{i=1}^{r} |x|_{[i]},\n",
    "        \\end{align*}$$\n",
    "    where $\\vert x \\vert_{[i]}$ is the $i$ th largest component of $|x_1|, \\cdots, |x_n|$. Decide whether $f(\\cdot)$ is convex or not. Prove your claim. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$\\bold{1}$. To prove this we need to prove that $g(\\lambda x_{1}+(1-\\lambda)x_{2})\\geq\\lambda g(x_{1})+(1-\\lambda)g(x_{2})$ for all $0 \\leq \\lambda \\leq 1$ and all $x_{1}, x_{2} \\in X$, which is the defintion of convexity.\n",
    "\n",
    "$g(\\lambda x_{1}+(1-\\lambda)x_{2})=$\n",
    "\n",
    "$f(A(\\lambda x_{1}+(1-\\lambda)x_{2})+b)=$\n",
    "\n",
    "$f(\\lambda(Ax_{1}+b)+(1-\\lambda)(Ax_{2}+b))$\n",
    "\n",
    "$f(\\lambda(Ax_{1}+b)+(1-\\lambda)(Ax_{2}+b))$ $\\geq$ $\\lambda f(Ax_{1}+b)+(1-\\lambda)f(Ax_{2}+b)$ from the definition of convexity on f. Continuing with the right hand side.\n",
    "\n",
    "$\\lambda f(Ax_{1}+b)+(1-\\lambda)f(Ax_{2}+b)=$\n",
    "\n",
    "$\\lambda g(x_{1}) + (1-\\lambda)g(x_{2})$\n",
    "\n",
    "Which means that we assembled the definition of convexity for g: \n",
    "\n",
    "$g(\\lambda x_{1}+(1-\\lambda)x_{2}) \\geq \\lambda g(x_{1}) + (1-\\lambda)g(x_{2})$\n",
    "\n",
    "$\\bold{2}$. The definition of the norm details 3 properties that have to be true in order for a function to be called a norm, we use the following two:\n",
    "\n",
    "$\\lvert x+y \\rvert \\leq \\lvert x \\rvert + \\lvert y \\rvert$ for all $x,y \\in X$ (Subadditivity)\n",
    "\n",
    "$\\lvert \\lambda x \\rvert = \\lvert \\lambda \\rvert \\lvert x \\rvert$ for all $x \\in X$ and $\\lambda \\in \\mathbb{R}$ (Homogeneity)\n",
    "\n",
    "Starting off from the left hand side of the definition for convexity $f(\\lambda x_{1}+(1-\\lambda)x_{2})$ for all $0 \\leq \\lambda \\leq 1$ and all $x_{1}, x_{2} \\in X$ and f is the norm.\n",
    "\n",
    "According to Subadditivity we have: $f(\\lambda x_{1}+(1-\\lambda)x_{2}) \\leq f(\\lambda x_{1}) + f((1-\\lambda)x_{2})$\n",
    "\n",
    "And applying Homogeneity gives us: $f(\\lambda x_{1}+(1-\\lambda)x_{2}) \\leq \\lambda f(x_{1}) + (1-\\lambda)f(x_{2})$, which is the definition \n",
    "of convexity for f.\n",
    "\n",
    "$\\bold{3}$. From subproblem 2 we know that the norm is convex, and combining this with subproblem 1 we know that $\\lVert  \\boldsymbol{A} \\boldsymbol{x} + \\boldsymbol{b} \\rVert$ is convex. \n",
    "\n",
    "In order to prove the problem statement we are going to prove that $h(x) = max\\{f_1(x),f_2(x)\\cdots f_n(x)\\}$ is convex if all $f_i$ are convex.\n",
    "\n",
    "$h(\\lambda x_{1}+(1-\\lambda)x_{2}) = f_i(\\lambda x_{1}+(1-\\lambda)x_{2})$ for some $i \\in \\{1,2,\\cdots,n\\}$ since $h$ is the maximum of all $f_i$.\n",
    "\n",
    "$f_i(\\lambda x_{1}+(1-\\lambda)x_{2}) \\leq \\lambda f_i(x_{1}) + (1-\\lambda)f_i(x_{2})$ from convexity of $f_i$.\n",
    "\n",
    "$\\lambda f_i(x_{1}) + (1-\\lambda)f_i(x_{2}) \\leq \\lambda h(x_{1}) + (1-\\lambda)h(x_{2})$ since $h(x) \\geq f_i(x)$ for all $x$ and $i$.\n",
    "\n",
    "Hence $h(\\lambda x_{1}+(1-\\lambda)x_{2}) \\leq \\lambda h(x_{1}) + (1-\\lambda)h(x_{2})$ and $h$ is convex.\n",
    "\n",
    "$\\bold{4}$. $f(x)$ can be thought of the r largest components of $x$ summed up. The sum of k specific components of a vector can be written as $\\sum^{k}_{i=1}x_{i} = a^Tx$ where $a_i = 1$ if $x_i$ is a component in x we want to include, for example if we want to sum the 1st and 3rd component of $x$ we have $a^T = (1,0,1,0,\\cdots,0)$.\n",
    "\n",
    "For any r, this is a finite set of functions and the sum of the r largest elements is the maximum of the set. As per subproblem 3 we know that the maximum of a set of convex functions is convex. Hence $f(x)$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Search\n",
    "\n",
    "### 6 (6%)\n",
    "Let $\\boldsymbol{x} \\in \\mathbb{R}^2$ and \n",
    "$$\\begin{align*}\n",
    "        f(\\boldsymbol{x}) = 3 |x_1| +  |x_2|.\n",
    "\\end{align*}$$\n",
    "Consider the point $\\boldsymbol{y} = [0,1]^{\\mathsf{T}}$. \n",
    "\n",
    "1.  Show that $\\boldsymbol{g} = [3,1]^{\\mathsf{T}} \\in \\partial f(\\boldsymbol{y})$. \n",
    "2.  Let $\\tau \\in (0,1)$, find the closed form for \n",
    "    $$\\begin{align*}\n",
    "        f(\\boldsymbol{y} - \\tau \\boldsymbol{g} ).\n",
    "    \\end{align*}$$\n",
    "3. Comment on whether $-\\boldsymbol{g}$ is a descent direction or not.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. $ùíà=[3,1]ùñ≥‚àà‚àÇùëì(\\boldsymbol{y})$.\n",
    "\n",
    "The subgradient at $\\boldsymbol{y} = [0,1]^{\\mathsf{T}}$ can be found as follows:\n",
    "$$\\partial f(0,1) = ([-3,3], 1) = \\{(g_1, 1) | g_1 \\in [-3,3]\\}$$\n",
    "Thus,\n",
    "$$\\boldsymbol{g} = [3,1]^{\\mathsf{T}} \\in \\partial f(0,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Let $\\tau \\in (0,1)$, find the closed form for $f(\\boldsymbol{y} - \\tau \\boldsymbol{g})$.\n",
    "\n",
    "1. When $0 \\lt \\tau \\le 1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f[(0,1)-\\tau(3,1)] &= f(3|-3\\tau| + |1 - \\tau|) \\\\\n",
    "    &= 9\\tau + 1 - \\tau\\\\\n",
    "    &= 8\\tau + 1\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "2. When $\\tau \\ge 1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f[(0,1)-\\tau(3,1)] &= f(3|-3\\tau| + |1 - \\tau|) \\\\\n",
    "    &= 9\\tau + \\tau - 1\\\\\n",
    "    &= 10\\tau - 1\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, closed form = \n",
    "$$\n",
    "f(\\boldsymbol{y} - \\tau \\boldsymbol{g})= \\begin{Bmatrix} 8\\tau +  \n",
    "      1, & 0 \\lt \\tau \\le 1\\\\ \n",
    "      10\\tau-1, & \\tau \\ge 1\\\\ \n",
    "   \\end{Bmatrix}  \n",
    "$$\n",
    "Therefore, required closed form = $8\\tau + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Comment on whether $-\\boldsymbol{g}$ is a descent direction or not.\n",
    "\n",
    "For a direction $\\boldsymbol{p}^l$ to be descent direction, it must satisfy the following:\n",
    "$$\\boldsymbol{p}^{l,T} \\nabla f(\\boldsymbol{x}^l) < 0 $$\n",
    "so that $$f(\\boldsymbol{x}^{l+1}) = f(x^{l} - \\tau g^{l}) < f(\\boldsymbol{x}^l)$$\n",
    "In other words,\n",
    "$$f(\\boldsymbol{x}^{l} - \\tau g^{l}) - f(\\boldsymbol{x}^l) < 0$$\n",
    "\n",
    "In the case when $0 \\lt \\tau \\le 1$:\n",
    "\n",
    "$$\\partial f(0,1) = \\{(g_1, 1) | g_1 \\in [-3,3]\\}$$\n",
    "$$\\boldsymbol{g} = [3,1]^{\\mathsf{T}} \\in \\partial f(0,1)$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f[(0,1)-\\tau(3,1)] - f(0,1) &= f(-3\\tau, 1-\\tau) - f(0,1) \\\\\n",
    "    &= (3|-3\\tau| + 1|1-\\tau|) - 0(3) - 1(1)\\\\\n",
    "    &= 9\\tau + 1 - \\tau -1 \\\\\n",
    "    &= 8\\tau\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $0 \\lt \\tau \\le 1$, $8\\tau > 0$.\n",
    "Thus, $f[(0,1)-\\tau(3,1)] - f(0,1)$ does not produce a negative descent, the direction $-\\textbf{g}$ is not a descent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 (10%)\n",
    "\n",
    "The following is the famous Wolfe's example, which shows that gradient descent method may not converge to a local optimal point.\n",
    "Let $\\boldsymbol{x}\\in \\mathbb{R}^2$ and \n",
    "$$\\begin{align*}\n",
    "        f(\\boldsymbol{x}) = \\begin{cases}\n",
    "            5(9x_1^2 + 16 x_2^2)^{1/2} & \\text{if}~ x_1 > |x_2|, \\\\\n",
    "            9x_1 + 16|x_2| & \\text{if}~ x_1 \\le |x_2|.\n",
    "        \\end{cases}\n",
    "    \\end{align*}$$\n",
    "Suppose that $\\boldsymbol{x}^0 = [16/9,1]^{\\mathsf{T}}$. Consider exact line search where \n",
    "    $$\\begin{align*}\n",
    "        \\boldsymbol{x}^{l+1} = \\boldsymbol{x}^l - t^{l+1} \\nabla f(\\boldsymbol{x}^l),\n",
    "    \\end{align*}$$\n",
    "    where \n",
    "    $$\\begin{align*}\n",
    "        t^{l+1} = \\arg~ \\min_t~ f(\\boldsymbol{x}^l - t \\nabla f(\\boldsymbol{x}^l)).\n",
    "    \\end{align*}$$\n",
    "\n",
    "\n",
    "1. Draw the contours of $f(\\boldsymbol{x})$ in the region $-2 \\le x_1 \\le 2$ and $-2 \\le x_2 \\le 2$.\n",
    "2. Is the point $\\boldsymbol{x} = [0,0]^{\\mathsf{T}}$ optimal? Why?\n",
    "3. Find the closed form of $\\nabla f(\\boldsymbol{x})$ in the region where $x_1 > |x_2|$.\n",
    "4.  Find $t^1$ and $\\boldsymbol{x}^1$. \n",
    "5. Find $t^2$ and $\\boldsymbol{x}^2$.\n",
    "6. Use mathematical induction, find $t^l$ and $\\boldsymbol{x}^l$. It can be concluded that $\\boldsymbol{x}^l \\rightarrow [0,0]^{\\mathsf{T}}$ as $l \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Draw Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "# Draw the contours of f(x) in the given region\n",
    "x_1 = -2:0.1:2;\n",
    "x_2 = -2:0.1:2;\n",
    "f(x_1, x_2) = if x_1 > abs(x_2) # Express the function\n",
    "            5 * sqrt(9 * x_1^2 + 16 * x_2^2)\n",
    "            elseif x_1<= abs(x_2)\n",
    "            9*x_1 + 16 * abs(x_2)\n",
    "            end\n",
    "X_1 = repeat(reshape(x_1, 1, :), length(x_2), 1)\n",
    "X_2 = repeat(x_2, 1, length(x_1))\n",
    "mapz = map(f, X_1, X_2)\n",
    "plotContour = contour(x_1, x_2, mapz, fill=false) # Plot contour\n",
    "plot(plotContour, xlabel=\"x_1\", ylabel=\"x_2\", plot_title=\"Contours of f\", plot_titlefontsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Is the point optimal?\n",
    "\n",
    "Since the  $-2 \\le x_1 \\le 2$,\n",
    "\n",
    "We can take the example of point $\\boldsymbol{x}_1 = [-1,0]^\\mathsf{T}$.\n",
    "\n",
    "$f(\\boldsymbol{x}_1) = -9$.\n",
    "\n",
    "This point already yields a smaller output compared to $f((0,0)) = 0$. Thus $[0,0]^\\mathsf{T}$ is not an optimal point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. Closed form of $\\nabla f(\\boldsymbol{x})$ in the region where $x_1 > |x_2|$.\n",
    "\n",
    "For range $ x_1 \\geq |x_2|$, $$f(x) = 5(9x_1^2 + 16x_2^2)^{\\frac{1}{2}}$$\n",
    "\n",
    "Differentiating w.r.t $x1$,\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\partial f_{x1}(x) &= \\frac{5}{2}(9x_1^2 + 16x_2^2)^{-\\frac{1}{2}} \\times \\frac{\\partial}{\\partial_{x1}}(9x_1^2 + 16x_2^2) \\\\\n",
    "     &= 45x_1(9x_1^2+16x_2^2)^{-\\frac{1}{2}}\n",
    "    \\end{aligned}\n",
    "$$\n",
    "Differentiating w.r.t $x2$,\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\partial f_{x2}(x) &= \\frac{5}{2}(9x_1^2 + 16x_2^2)^{-\\frac{1}{2}} \\times \\frac{\\partial}{\\partial_{x2}}(9x_1^2 + 16x_2^2) \\\\\n",
    "     &= 80x_2(9x_1^2+16x_2^2)^{-\\frac{1}{2}}\n",
    "    \\end{aligned}\n",
    "$$\n",
    "Therefore, the closed form of $\\nabla f(x)$, at the points where $ x_1 \\geq |x_2|$, is:\n",
    "    $$\n",
    "        \\nabla f(x) = [45x_1(9x_1^2+16x_2^2)^{-\\frac{1}{2}}, \\space 80x_2(9x_1^2+16x_2^2)^{-\\frac{1}{2}}]^T\\tag{1}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4. Find $t^1$ and $\\boldsymbol{x}^1$. \n",
    "\n",
    "Line search method:\n",
    "$$\n",
    "    \\boldsymbol{x}^{l+1} = \\boldsymbol{x}^l + t^l\\boldsymbol{p}^l \\tag{2} ,\n",
    "$$\n",
    "where search direction $\\boldsymbol{p}^l$ and step size $t^l > 0$ are chosen so that $f(\\boldsymbol{x}^l + t^l\\boldsymbol{p}^l) < f(\\boldsymbol{x}^l)$.\n",
    "\n",
    "Considering that search direction, $\\boldsymbol{p}^l = -\\nabla f(\\boldsymbol{x}^l) ,$ step size can be found as follows.\n",
    "$$\n",
    "    t^l = \\arg~ \\min_t~ f(x^l - t \\nabla f(x^l))\n",
    "$$\n",
    "\n",
    "Given $\\boldsymbol{x}^0 = [16/9,1]^{\\mathsf{T}}$,\n",
    "\n",
    "$$\n",
    "    \\nabla f(x) = [45x_1(9x_1^2+16x_2^2)^{-\\frac{1}{2}}, \\space 80x_2(9x_1^2+16x_2^2)^{-\\frac{1}{2}}]^T \\\\\n",
    "    \\nabla f(x^0) = \\left[45\\times\\frac{16}{9}\\left(9\\times(\\frac{16}{9})^2+16\\times 1^2\\right)^{-\\frac{1}{2}}, \\space 80\\times 1\\left(9\\times (\\frac{16}{9})^2+16\\times 1^2\\right)^{-\\frac{1}{2}}\\right]^T \\\\\n",
    "    = [12, 12]^T\n",
    "$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "    x^0 - t \\nabla f(x^0) = \\left[\\frac{16}{9},\\space 1\\right]^T - t[12, 12]^T = \\left[\\frac{16}{9} - 12t, \\space 1-12t\\right]^T\n",
    "$$\n",
    "\n",
    "Susbtituting, we get\n",
    "$$\n",
    "    f(x^0 - t \\nabla f(x^0)) = 3600t^2 - 768t + \\frac{400}{9}\n",
    "$$\n",
    "$$t^1 = \\arg~ \\min_t~ f(x^0 - t \\nabla f(x^0))$$\n",
    "\n",
    "Comparing $f$ when $x_1 > |x_2|$ and $x_1 \\le |x_2|$, minimum value was found when $x_1 > |x_2|$.\n",
    "\n",
    "In order to minimise, we must get gradient = 0.\n",
    "$$\n",
    "    \\frac{d}{dt}\\left(3600t^2 - 768t + \\frac{400}{9}\\right)=7200t-768=0\n",
    "$$\n",
    "Thus, $$t_1 = \\frac{768}{7200} = \\frac{8}{75}$$\n",
    "Thus,\n",
    "$$\n",
    "    x^1 = x^0 - t \\nabla f(x^0) = \\left[\\frac{16}{9},\\space 1\\right]^T - \\frac{8}{75}[12, 12]^T \\\\\n",
    "    = \\left[\\frac{16}{9} - 12\\times \\frac{8}{75}, \\space 1-12\\times \\frac{8}{75}\\right]^T \\\\\n",
    "    = \\left[\\frac{112}{225}, -\\frac{7}{25}\\right]^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5. Find $t^2$ and $x^2$\n",
    "\n",
    "$t^2$ and $x^2$ can be found with the similar steps shown above. Taking $x^1 = \\left[\\frac{112}{225}, -\\frac{7}{25}\\right]^T$,\n",
    "\n",
    "$$\n",
    "    \\nabla f(x) = [45x_1(9x_1^2+16x_2^2)^{-\\frac{1}{2}}, \\space 80x_2(9x_1^2+16x_2^2)^{-\\frac{1}{2}}]^T \\\\\n",
    "    \\nabla f(x^1) = \\left[45\\times\\frac{112}{225}\\left(9\\times\\frac{112}{225}^2+16\\times (-\\frac{7}{25})^2\\right)^{-\\frac{1}{2}}, \\space 80\\times -\\frac{7}{25}\\left(9\\times \\frac{112}{225}^2+16\\times (-\\frac{7}{25})^2\\right)^{-\\frac{1}{2}}\\right]^T\n",
    "        = [12, -12]^T\n",
    "$$\n",
    "\n",
    "Substituting back,\n",
    "\n",
    "$$\n",
    "    x^1 - t \\nabla f(x^1) = \\left[\\frac{112}{225}, -\\frac{7}{25}\\right]^T - t[12, -12]^T = \\left[\\frac{112}{225}-12t, -\\frac{7}{25}+12t\\right]^T\n",
    "$$\n",
    "\n",
    "Comparing $f$ when $x_1 > |x_2|$ and $x_1 \\le |x_2|$, minimum value was found when $x_1 > |x_2|$. \n",
    "\n",
    "Now, minimise $f$ to find $t^2$.\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt}\\left(3600t^2 - \\frac{5376}{25}t + \\frac{784}{225}\\right)=7200t-\\frac{5376}{25}=0 \\\\\n",
    "t^2 = \\arg~ \\min_t~ f(x^1 - t \\nabla f(x^1)) = \\frac{5376}{25(7200)} =  \\frac{56}{1875} \\\\\n",
    "$$\n",
    "\n",
    "Therefore, substituting the values found,\n",
    "\n",
    "$$\n",
    "    x^2 = x^1 - t^{2} \\nabla f(x^1) = \\left[\\frac{784}{5625},\\space \\frac{49}{625}\\right]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Gradient Method\n",
    "\n",
    "### 8 (12%)\n",
    "For a given function $f$ and a given constant $\\gamma >0$, the proximal operator is given by\n",
    "\n",
    "$$\\begin{equation*}\n",
    "    \\bm{x}^{\\star} = \\text{Prox}_{\\gamma f}(\\bm{z}) := \\arg~ \n",
    "    \\min_{\\bm{x}}~ f(\\bm{x}) \n",
    "    + \\frac{1}{2 \\gamma} \\left\\| \\bm{x} - \\bm{z} \\right\\|_2^2.\n",
    "\\end{equation*}$$\n",
    "\n",
    "Find the closed form of the proximal operator $\\text{Prox}_{\\gamma f}(\\bm{z})$ for the following functions $f$. Give the key steps for your derivation.  \n",
    "1. $f(\\bm{x}) = \\delta( \\bm{x} \\le u ) $ (each element of $x$ is at most $u$). \n",
    "2. $f(\\bm{x}) = \\delta( l \\le \\bm{x} \\le u ) $ (each element of $x$ is in $[l,u]$).\n",
    "3. $f(\\bm{x}) = \\delta( \\lVert \\bm{x} \\rVert_{\\infty} \\le a ) $. (See Lecture Notes Example 5.1 for the definition of $\\lVert \\cdot \\rVert_{\\infty}$). \n",
    "4. $f(\\bm{x}) = \\frac{1}{2} \\lVert \\bm{A} \\bm{x} - \\bm{b} \\rVert^2 $. \n",
    "5. $f(\\bm{x}) = \\lVert \\bm{x} \\rVert$. \n",
    "6. $f(\\bm{x}) = \\delta( \\lVert \\bm{x} \\rVert \\le a )$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1\n",
    "\n",
    "The value of $f(x)$ is $+\\infty$ if $x$ does not statisfy the condition of the indicator function, cannot be a function minimum (since vectors exist which statisfy the condition), so $x^*$ has to be inside the domain where the vectors statisfy the condition. The second term of the proximal operator is smallest when the euclidian distance between $x^*$ and $z$ is smallest, so $x^*$ is the closest point to $z$ in the domain where the vectors statisfy the condition, also known as the projection of $z$ onto the domain.\n",
    "\n",
    "The domain can be visualised in 2 dimensions as an infinite squre where the top right corner is in $(u,u)$, it is easy to see that the shortest distance between $z$ and the domain is going to be a straight line if to the square if only one coordinate is larger than $u$ and a line to the corner if both coordinates are larger than $u$.\n",
    "\n",
    "For this specific case any values of $z$ that are larger than $u$ will be projected to $u$, and any values of Z that are smaller than $u$ will be unaffected. \n",
    "\n",
    "The solution therefore is $x^*$ where $x^*_i$ (the ith element of $x*$) $= \\begin{cases} u & \\text{if}~ z_i > u \\\\ z_i & \\text{if}~ z_i \\le u \\end{cases}$\n",
    "\n",
    "### 8.2\n",
    "\n",
    "Similarly the solution is going to be a projection of $z$ onto the domain where the vectors statisfy the condition.\n",
    "\n",
    "The domain (in 2D) in this case is going to be a square, where the bottom left corner is in $(l,l)$ and the top right corner is in $(u,u)$.\n",
    "\n",
    "The solution therefore is $x^*$ where $x^*_i$ (the ith element of $x*$) $= \\begin{cases} u & \\text{if}~ z_i > u \\\\ z_i & \\text{if}~ l \\le z_i \\le u \\\\ l & \\text{if}~ z_i \\lt l \\end{cases}$\n",
    "\n",
    "### 8.3\n",
    "\n",
    "$\\lVert \\bm{x} \\rVert_{\\infty} \\le a$ is equivalent to stating that no element of $x$ has a larger absolute value (if $a \\lt 0$ there is no solution since the value of x on the whole domain is $+\\infty$) $a$, once again using the fact that the proximal operator of the indicator function is the projection of $z$ onto the domain where the vectors statisfy the condition.\n",
    "\n",
    "The domain (in 2D) in this case is going to be a square, where the bottom left corner is in $(-a,-a)$ and the top right corner is in $(a,a)$.\n",
    "\n",
    "The solution therefore is $x^*$ where $x^*_i$ (the ith element of $x*$) $= \\begin{cases} a & \\text{if}~ z_i > a \\\\ |z_i| & \\text{if}~ -a \\le z_i \\le a \\\\ -a & \\text{if}~ z_i \\lt -a \\end{cases}$\n",
    "\n",
    "### 8.4\n",
    "$\\nabla(\\frac{1}{2}\\lVert \\bm{A} \\bm{x} - \\bm{b} \\rVert^2) = A^T(Ax-b)$\n",
    "\n",
    "$\\nabla(\\frac{1}{2 \\gamma}\\lVert \\bm{x} - \\bm{b} \\rVert^2) = \\frac{x-b}{\\gamma}$\n",
    "\n",
    "Differentiating both sides with respect to $x_j$ and setting it to zero gives the solution to the problem.\n",
    "$arg_{x} A^{T}(Ax - b) + \\frac{x-z}{\\gamma} = 0$\n",
    "\n",
    "### 8.5\n",
    "\n",
    "We use the Moreau decomposition, which states that $\\text{Prox}_{\\gamma f}(z) = z - \\gamma \\text{Prox}_{\\frac{f^{*}}{\\gamma}}(\\frac{z}{\\gamma})$ where $f^{*}$ is the convex conjugate of $f$. \n",
    "\n",
    "The convex conjugate of the $\\ell_2$ norm (f) is the indicator function of the $\\ell_2$ unit ball, so $f^{*}(z) = \\delta(\\lVert \\bm{z} \\rVert \\le 1)$, which is the same as $f^{*}(z) = i_{\\lVert \\bm{x} \\rVert \\le 1} = \\begin{cases} 0 & \\text{if}~ \\lVert \\bm{z} \\rVert \\le 1 \\\\ +\\infty & \\text{if}~ \\lVert \\bm{z} \\rVert \\gt 1 \\end{cases}$\n",
    "\n",
    "As previously stated the proximal operator of the indicator function is the projection of $z$ onto the domain where the vectors statisfy the condition. The domain in this case is the unit $\\ell_2$ ball, which is an N-sphere with radius $1$, centered at the origin.\n",
    "\n",
    "The projection of z onto the unit N-sphere is given by $\\text{Prox}_{f^*}(z) = \\begin{cases} \\frac{z}{\\lVert z \\rVert} & \\text{if}~ \\lVert \\bm{z} \\rVert \\gt 1 \\\\ z & \\text{if}~ \\lVert \\bm{z} \\rVert \\leq 1 \\end{cases}$ (since it is the unit vector pointing towards z, times the radius of the circle $1$).\n",
    "\n",
    "$\\text{Prox}_{\\gamma f}(z) = z - \\gamma \\text{Prox}_{\\frac{f^{*}}{\\gamma}}(\\frac{z}{\\gamma}) = \\begin{cases} z - \\gamma \\frac{z}{\\lVert z \\rVert} & \\text{if}~ \\lVert z \\rVert \\geq \\gamma \\\\ z & \\text{if}~ \\lVert z \\rVert \\lt \\gamma \\end{cases}$\n",
    "\n",
    "\n",
    "### 8.6\n",
    "\n",
    "Similarly the solution is going to be a projection of $z$ onto the domain where the vectors statisfy the condition.\n",
    "\n",
    "The domain is going to be an N-sphere with radius $a$, centered at the origin since the vectors that statisfy the condition have their euclidean distance from the origin be at most $a$.\n",
    "\n",
    "we can reuse our formula for the projection of z onto the unit N-sphere, but this time we multiply it by $a$ to get the projection of z onto the N-sphere with radius $a$ instead of $1$.\n",
    "\n",
    "$\\text{Prox}_{f}(z) = \\begin{cases} a\\frac{z}{\\lVert z \\rVert} & \\text{if}~ \\lVert \\bm{z} \\rVert \\gt a \\\\ |z| & \\text{if}~ \\lVert \\bm{z} \\rVert \\leq a \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 (14%)\n",
    "Suppose that we know how to compute the proximal operator $\\text{Prox}_{\\gamma g}(\\bm{z})$. Find the form of the proximal operator $\\text{Prox}_{\\gamma f}(\\bm{z})$ in terms of $\\text{Prox}_{\\gamma g}(\\bm{z})$ for the following functions $f$. Give the key steps for your derivation. \n",
    "\n",
    "1.  $f(\\bm{x}) = g(\\bm{x}) + \\frac{\\rho}{2} \\lVert \\bm{x} - \\bm{b} \\rVert^2$ where $\\rho > 0$. \n",
    "2.  $f(\\bm{x}) = g(\\bm{x}) + \\langle \\bm{a},\\bm{x} \\rangle$.\n",
    "3.  $f(\\bm{x}) = a g(\\bm{x}) + b$ where $a>0$.\n",
    "4.  $f(\\bm{x}) = g(\\bm{x} + b)$.\n",
    "5.  $f(\\bm{x}) = g(a\\bm{x})$ where $a>0$.\n",
    "6.  $f(\\bm{x}) = g(\\bm{A}\\bm{x})$ where $\\bm{A}$ is orthonormal, i.e., $\\bm{A}^{\\mathsf{T}}\\bm{A} = \\bm{A} \\bm{A}^{\\mathsf{T}} = \\bm{I}$.\n",
    "7.  $f(\\bm{x}) = g(a\\bm{A}\\bm{x})$ where $a>0$ and $\\bm{A}$ is orthonormal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1\n",
    "\n",
    "Using the regularization property of the proximal operator, which states that if $f(x) = \\phi(x) + \\frac{\\rho}{2} \\lVert x - b \\rVert^2$ then $\\text{Prox}_{\\gamma f}(v) = \\text{Prox}_{\\hat{\\gamma} \\phi}(\\frac{\\hat{\\gamma}}{\\gamma}v+(\\rho \\hat{\\gamma})a)$ where $\\hat{\\gamma}=\\frac{\\gamma}{(1+\\gamma \\rho)}$.\n",
    "\n",
    "It follows that $\\text{Prox}_{\\gamma f}(z) = \\text{Prox}_{\\hat{\\gamma} g}(\\frac{\\hat{\\gamma}}{\\gamma}z+(\\rho \\hat{\\gamma})b)$\n",
    "\n",
    "### 9.2\n",
    "\n",
    "The affine addition property of the proximal operator states that if $f(x) = \\phi(x) + \\langle a,x \\rangle + b$ then $\\text{Prox}_{\\gamma f}(z) = \\text{Prox}_{\\gamma \\phi}(z - \\gamma a)$\n",
    "\n",
    "### 9.3\n",
    "\n",
    "The postcomposition property of the proximal operator states that if $f(x) = a \\phi(x) + b$ then $\\text{Prox}_{\\gamma f}(z) = \\text{Prox}_ {\\gamma a \\phi}(z)$\n",
    "\n",
    "### 9.4\n",
    "The precomposition property of the proximal operator states that if $f(x) = \\phi(ax + b)$ with $a\\neq 0$ then $\\text{Prox}_{\\gamma f}(z) = \\frac{1}{a}(\\text{Prox}_{a^{2} \\gamma \\phi}(az+b)-b)$, taking the special case where $a=1$ we get $\\text{Prox}_{\\gamma f}(z) = \\text{Prox}_{\\gamma \\phi}(z-b)-b$\n",
    "\n",
    "### 9.5\n",
    "Similarly to 9.4 except the special case being $b=0$. $\\text{Prox}_{\\gamma f}(z) = \\frac{1}{a}(\\text{Prox}_{a^2 \\gamma \\phi}(az))$\n",
    "\n",
    "### 9.6\n",
    "According to the precomposition property of the proximal operator, if $f(x) = \\phi(Ax)$ then $\\text{Prox}_{\\gamma f}(z) = A^T \\text{Prox}_{\\gamma \\phi}(Az)$\n",
    "\n",
    "### 9.7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###   10 MRI Compressed Sensing Recovery via Proximal Gradient (24%)\n",
    "\n",
    "Let $\\bm{X}$ be an MRI image. A typical MRI image is sparse under wavelet transform. An MRI machine takes samples in the frequency domain. \n",
    "\n",
    "\n",
    "*  We verify that an MRI image is sparse under wavelet transform as follows. (6%)\n",
    "\n",
    "    1.  Use the package `Images` to load `MRI_of_Human_Brain.jpg`. Show the MRI image that you load. Denote the image by `X0`. \n",
    "\n",
    "    2.  Use the package `Wavelets` to perform 2D Daubechies wavelet transform of the MRI image. Show the wavelet coefficients as an image. (See `Github>Wavelets.jl/example/transform2d` for an example.) Denote the wavelet coefficients by `W0`.\n",
    "    \n",
    "    3.  Keep 20% wavelet coefficients that are of the largest magnitudes and set the rest wavelet coefficients to zero. Denote the resulting 'truncated' wavelet coefficients by `W1`. Perform inverse 2D Daubechies wavelet transform to obtain an image `X1`. This image should look like the image `X0` with minor quality loss. \n",
    "\n",
    "    \n",
    "* Fourier domain compressed sensing: (6%)\n",
    "\n",
    "    4.  Use the package `FFTW` to perform 2D Fourier transform of `X0`. Denote the obtained Fourier coefficients by `F0`. \n",
    "\n",
    "    5. Set the random seed to 0. Generate the random sampling index set by using `StatsBase.sample` and denote it by $\\Omega$ such that the size of $\\Omega$ is 30% of the size of `X0`. \n",
    "\n",
    "    6.  Let $y = \\mathcal{P}_{\\Omega} ( \\mathcal{F}(X0))$. It is our MRI measurement vector. \n",
    "\n",
    "\n",
    "* Compressed sensing recovery. We are going to use proximal gradient method to solve the optimization problem. We are going to denote the recovered image by `Xhat`. (12%) \n",
    "\n",
    "    $$\\begin{align*}\n",
    "        & \\min_{X}~ F(X) := \\lambda \\lVert \\mathcal{W}(X) \\rVert_1 + \\frac{1}{2} \\lVert y - \\mathcal{P}_{\\Omega} (\\mathcal{F}(X)) \\rVert_2^2.\n",
    "    \\end{align*}$$ \n",
    "    \n",
    "    7.  Use the results that you obtained in coursework problem related to proximal operators to obtain the form of the proximal operator for the function $g(X) = \\lambda \\lVert \\mathcal{W}(X) \\rVert_1$. \n",
    "\n",
    "    8.  Write the objective function $F(X) = f(X) + g(X)$ where $f(X)$ is differentiable. Find the closed form for $\\nabla f(X)$ (Recall the adjoint of linear operator $\\mathcal{A}$ is denoted by $\\mathcal{A}^*$). \n",
    "    \n",
    "    9.  Implement the proximal gradient method to recover the image from $y$. In your codes, use comments to indicate clearly which part is to compute $\\nabla f(X)$, which part is to perform proximal operator, and which part is to find a valid step size. \n",
    "\n",
    "\n",
    "    10. Test the performance. Now find a good value of $\\lambda$ so that the recovery result looks nice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########1#########\n",
    "using Images,ImageView,Wavelets\n",
    "X0 = load(\"MRI_of_Human_Brain.jpg\");\n",
    "imshow(X0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######2#########\n",
    "x = Float64.(X0);\n",
    "W0 = dwt(x, wavelet(WT.db2))\n",
    "imshow(W0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######3#######\n",
    "(c,k) = size(W0);\n",
    "allp = c*k;\n",
    "allzero = 0.2*allp;\n",
    "h = 1;\n",
    "l = 1;\n",
    "old = 0;\n",
    "W1 = zeros(c,k);\n",
    "for i in 1:allzero\n",
    "    for a in 1:c\n",
    "        for b in 1:k\n",
    "            if W0[a,b]>old\n",
    "                h = copy(a);\n",
    "                l = copy(b);\n",
    "                old = copy(W0[a,b]);           \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    W1[h,l] = copy(W0[h,l]);\n",
    "    W0[h,l] = 0;\n",
    "    old = 0;\n",
    "end\n",
    "X1 = idwt(W1, wavelet(WT.db2))\n",
    "imshow(X1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######4#######\n",
    "using FFTW\n",
    "using Images,ImageView\n",
    "X0 = load(\"MRI_of_Human_Brain.jpg\");\n",
    "X0 = Float64.(X0);\n",
    "F0 = fft(X0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######5#######\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools\n",
    "using Base\n",
    "using Random\n",
    "using Pkg\n",
    "Pkg.add(\"StatsBase\")\n",
    "using StatsBase\n",
    "Random.seed!(0)\n",
    "(c,k) = size(X0);\n",
    "f0 = zeros(ComplexF64,c,k);\n",
    "Omega = sample(collect(1:70144),21043);\n",
    "Omega = sort(Omega); \n",
    "o = zeros(21043, 70144);\n",
    "j = 1;\n",
    "for i in Omega\n",
    "    o[j, i] = 1;\n",
    "    j = j +1;\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######6#######\n",
    "function dft2d_matrix(M::Int64,N::Int64)\n",
    "    leftdftM = Array{ComplexF64,2}(undef,M,M);\n",
    "    rightdftM = Array{ComplexF64,2}(undef,N,N);\n",
    "    leftdftMinv = copy(leftdftM); \n",
    "    rightdftMinv = copy(rightdftM); \n",
    "    for q in 1:M \n",
    "        for p in 1:M\n",
    "            leftdftM[p,q] = exp(-im*2pi/M*(p-1)*(q-1))\n",
    "            leftdftMinv[p,q] = exp(im*2pi/M*(p-1)*(q-1))\n",
    "        end\n",
    "    end\n",
    "    leftdftMinv = leftdftMinv/M; \n",
    "\n",
    "    for q in 1:N \n",
    "        for p in 1:N\n",
    "            rightdftM[p,q] = exp(-im*2pi/N*(p-1)*(q-1))\n",
    "            rightdftMinv[p,q] = exp(im*2pi/N*(p-1)*(q-1))\n",
    "        end\n",
    "    end\n",
    "    rightdftMinv = rightdftMinv/N; \n",
    "    return leftdftM,leftdftMinv,rightdftM,rightdftMinv\n",
    "    \n",
    "end\n",
    "leftdftM,leftdftMinv,rightdftM,rightdftMinv = dft2d_matrix(c,k);\n",
    "y = o*vec(leftdftM*X0*rightdftM);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####8#####\n",
    "\n",
    "$y=\\Omega \\cdot vec\\left( F1\\cdot X0\\cdot F2\\right)$\n",
    "\n",
    "$\\nabla f(x)=-\\left( \\Omega \\cdot F^{T}_{2}\\otimes F_{1}\\right)^{T}  \\left( \\hat{y}-\\Omega \\cdot F^{T}_{2}\\otimes F_{1}\\cdot vec\\left( X_{0}\\right)  \\right)$\n",
    "\n",
    "$=-F_{2}\\otimes F^{T}_{1}\\cdot \\Omega^{T} \\cdot \\hat{y} +F_{2}\\otimes F^{T}_{1}\\cdot \\Omega^{T} \\cdot \\Omega \\cdot vec\\left( F_{1}\\cdot X_{0}\\cdot F_{2}\\right)  $\n",
    "\n",
    "$=-vec\\left( F^{T}_{1}\\cdot reshape(\\Omega^{T} \\hat{y})\\cdot F^{T}_{2}\\right)  +vec\\left( F^{T}_{1}\\cdot reshape(\\Omega^{T} \\cdot \\Omega \\cdot vec\\left( F_{1}\\cdot X_{0}\\cdot F_{2}\\right)  )\\cdot F^{T}_{2}\\right)  $ \n",
    "\n",
    "$u=x^{l-1}-\\gamma \\cdot \\nabla f(x)$\n",
    "\n",
    "######7#####\n",
    "\n",
    "$z=\\begin{cases}u+\\lambda \\gamma &u\\leqslant -\\lambda \\gamma \\\\ 0&u<\\left| \\lambda \\gamma \\right|  \\\\ u-\\lambda \\gamma &u\\geqslant \\lambda \\gamma \\end{cases} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####9#####\n",
    "lamda = 0.2;\n",
    "gama = 1;\n",
    "yita = 0.5;\n",
    "oldx = randn(70144,1);\n",
    "z = 0;\n",
    "qian = 100;\n",
    "while qian+lamda*norm(W0,1)>0.1\n",
    "    newgama = gama;\n",
    "    while true\n",
    "        #######compute df(X)#######\n",
    "        ou = -vec(leftdftM'*reshape(o'*y,(274,256))*rightdftM')+vec(leftdftM'*reshape(o'*o*vec(leftdftM*reshape(oldx,(274,256))*rightdftM),(274,256))*rightdftM');\n",
    "        u =  oldx-newgama*ou;\n",
    "        #######perform proximal operator#######\n",
    "        z = max.((abs.(u).-lamda*newgama),0);\n",
    "        qian = 0.5*sum((y - o*vec(leftdftM*reshape(z,(274,256))*rightdftM)).^2);\n",
    "        #println(qian);\n",
    "        hou = [0.5*sum((y - o*vec(leftdftM*reshape(oldx,(274,256))*rightdftM)).^2)]+(z-oldx)'*ou+[1/newgama*0.5*sum((z-oldx).^2)];\n",
    "        #println(hou[1]);\n",
    "        if abs(qian)<=abs(hou[1])\n",
    "            break;\n",
    "        end\n",
    "        #######find a valid step size#######\n",
    "        newgama = yita*newgama;\n",
    "    end\n",
    "    println(abs(newgama-gama));\n",
    "    gama = newgama;\n",
    "    oldx = z;\n",
    "    println(sum((z-vec(X0)).^2));\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(reshape(z,(274,256)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0de548ffb181fe2a01b2ed5f402e6bb6e6ccf2641efffae0817d36d41e1e10e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
